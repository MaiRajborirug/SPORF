{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and visualize EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import h5py\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths\n",
    "base_dir = Path('/mnt/ssd3/ronan/grasp-and-lift-eeg-detection')\n",
    "raw_dir = base_dir / 'raw' / 'train'\n",
    "\n",
    "# Columns name for labels\n",
    "cols = ['HandStart','FirstDigitTouch',\n",
    "        'BothStartLoadPhase','LiftOff',\n",
    "        'Replace','BothReleased']\n",
    "\n",
    "# Number of subjects\n",
    "subjects = range(1,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(fname):\n",
    "    \"\"\" read and prepare training data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    # events file\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#remove id\n",
    "    labels=labels.drop(['id' ], axis=1)#remove id\n",
    "    return  clean,labels\n",
    "\n",
    "def get_raw_data(data_dir, subjects):\n",
    "    y = []\n",
    "    X = []\n",
    "    sub_list = []\n",
    "    for i,subject in enumerate(subjects):\n",
    "        ################ READ DATA ################################################\n",
    "        fnames =  glob(str(data_dir / f'subj{subject}_series*_data.csv'))\n",
    "        for fname in fnames:\n",
    "            data,labels=prepare_data_train(fname)\n",
    "            X.append(np.asarray(data))\n",
    "            y.append(np.asarray(labels))\n",
    "            sub_list.append(subject)\n",
    "\n",
    "    return (X, y, np.asarray(sub_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_regions(a, val=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : 1-D array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Returns a list of regions where value `val` is in array `a`\n",
    "    Regions in a named tuple of start index and length\n",
    "    \"\"\"\n",
    "    Region = namedtuple('Region', ['start', 'length'])\n",
    "    regions = []\n",
    "    hit = False\n",
    "    start_idx = None\n",
    "    for i,d in enumerate(a):\n",
    "        if d == val and hit == False:\n",
    "            hit = True\n",
    "            start_idx = i\n",
    "        elif not d == val and hit == True:\n",
    "            hit = False\n",
    "            regions.append(Region(start_idx, i - start_idx))\n",
    "            \n",
    "    if hit == True:\n",
    "        regions.append(Region(start_idx, len(a) - start_idx))\n",
    "        \n",
    "    return regions\n",
    "\n",
    "def get_region_dict(data, cols):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2-D array-like of labels\n",
    "        rows = timesteps, cols = classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Returns dictionary of regions for each classification label \n",
    "    in `cols` and `None`\n",
    "    \"\"\"\n",
    "    region_dict = dict()\n",
    "    zeros_all = np.sum(data, axis=1)\n",
    "    \n",
    "    region_dict['None'] = consecutive_regions(zeros_all, val=0)\n",
    "    \n",
    "    for label, array in zip(cols, data.T):\n",
    "        region_dict[label] = consecutive_regions(array, val=1)\n",
    "        \n",
    "    return region_dict\n",
    "\n",
    "def sample_active_sites(regions, data, label, prelag=500, postlag=500):\n",
    "    \"\"\"\n",
    "    Samples regions from motion sides with specified lags\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2-D array-like\n",
    "        rows = sensors, cols = timesteps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : array of flattened (#channels x #timesteps) matrix with \n",
    "        prelag steps before action and postlag steps after\n",
    "    y : class label for each flattened matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for region in regions:\n",
    "        if region.start >= prelag:\n",
    "            X.append(data.T[:, region.start-prelag: region.start+postlag].flatten())\n",
    "            y.append(label)\n",
    "            \n",
    "    return(np.asarray(X), np.asarray(y))\n",
    "\n",
    "def sample_inactive_sites(regions, data, label, prelag=500, postlag=500):\n",
    "    \"\"\"\n",
    "    Samples max number of independent regions of inactive sites\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2-D array-like\n",
    "        rows = sensors, cols = timesteps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : array of flattened (#channels x #timesteps) matrix with \n",
    "        prelag steps before action and postlag steps after\n",
    "    y : class label for each flattened matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for region in regions:\n",
    "        lag = (prelag + postlag)\n",
    "        num_intervals = int(region.length / lag)\n",
    "        for n in range(num_intervals - 1):\n",
    "            X.append(data.T[:, region.start+postlag+n*lag: region.start+postlag+(n+1)*lag].flatten())\n",
    "            y.append(label)\n",
    "            \n",
    "    return(np.asarray(X), np.asarray(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Features\n",
    "Pipeline per subject\n",
    "1. Load all series from a subject (series x timesteps x channels)\n",
    "2. Convert each series to class class (series x class x tuples(start, length))\n",
    "3. Sample active sites from regions (series*class x (channels x timesteps))\n",
    "4. Concatenate samples from each series (X,y)\n",
    "5. Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject to load\n",
    "subject = 1\n",
    "prelag = 500\n",
    "postlag = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /mnt/ssd3/ronan/grasp-and-lift-eeg-detection/raw/train\n"
     ]
    }
   ],
   "source": [
    "# -> (series x (timesteps x channels))\n",
    "print(f'Loading data from {raw_dir}')\n",
    "X_raw,y_raw,subs = get_raw_data(raw_dir, [subject])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_raw = [scaler.fit_transform(Xr) for Xr in X_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> (series x dict{class, tuples(start, length)})\n",
    "region_dicts = [get_region_dict(y_series, cols) for y_series in y_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> (class*series x (channels x timesteps))\n",
    "X = []\n",
    "y = []\n",
    "for series, region_dict in enumerate(region_dicts):\n",
    "    Xi, yi = sample_active_sites(region_dict['None'], X_raw[series], label=0, prelag=prelag, postlag=postlag)\n",
    "    X.append(Xi)\n",
    "    y.append(yi)\n",
    "    for i,key in enumerate(cols):\n",
    "        Xi, yi = sample_active_sites(region_dict[key], X_raw[series], label=i+1, prelag=prelag, postlag=postlag)\n",
    "        X.append(Xi)\n",
    "        y.append(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate samples from each series -> (X,y)\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2430, 32000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "save_dir = base_dir / 'processed'\n",
    "\n",
    "if not os.path.exists(save_dir): \n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "h5f = h5py.File(save_dir / f'subj{subject}_prelag={prelag}_postlag={postlag}_Xy.hdf5', 'w')\n",
    "h5f.create_dataset('X', data=X)\n",
    "h5f.create_dataset('y', data=y)\n",
    "\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_rerf",
   "language": "python",
   "name": "ts_rerf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
