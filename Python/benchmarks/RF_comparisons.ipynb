{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7a17de15524c6f9a541dabe8cc9384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=69), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openml\n",
    "from benchmark_utils import *\n",
    "\n",
    "benchmark_suite = openml.study.get_study('OpenML-CC18','tasks') # obtain the benchmark suite\n",
    "M = len(benchmark_suite.tasks)\n",
    "\n",
    "# The number of times to run each algorithm\n",
    "n_iterations = 10\n",
    "\n",
    "# RerF types to benchmark, the duplicate is to allow for RerFs with different\n",
    "# parameters\n",
    "rerfs = ['rfBase', \n",
    "         'binnedBase',\n",
    "    ]\n",
    "\n",
    "# RerF parameters\n",
    "rerf_kwargs = [\n",
    "    {\n",
    "    'trees' : 10,\n",
    "    'numCores' : 8,\n",
    "    'minParent' : 25,\n",
    "    },\n",
    "#     {\n",
    "#     'trees' : 20,\n",
    "#     'mtryMult': 2, # Comment this line out if RerF version does not support mtryMult\n",
    "#     'numCores' : 8,\n",
    "#     }\n",
    "    ]\n",
    "\n",
    "# Keyword of the parameter we are changing/iterating over\n",
    "rerf_param_keyword='trees'\n",
    "\n",
    "# Sklearn forest types to benchmark\n",
    "sklearns = ['RandomForest', 'ExtraTrees']\n",
    "\n",
    "# If all forests share the same parameters only need to pass single dictionary \n",
    "sklearn_kwargs = [{\n",
    "    'n_estimators' : 10,\n",
    "    'n_jobs' : 8,\n",
    "    'min_samples_split' : 25\n",
    "}]\n",
    "\n",
    "# Sklearn keyword of the parameter we are changing/iterating over\n",
    "sklearn_param_keyword='n_estimators'\n",
    "\n",
    "# The prameter values to consider \n",
    "# param_values = np.arange(20, 101, step=20)\n",
    "param_values = np.arange(10, 1001, step = 20)\n",
    "\n",
    "# A list of all forest types (duplicates included)\n",
    "all_forests = np.concatenate((rerfs, sklearns))\n",
    "F = len(all_forests)\n",
    "\n",
    "predictions = [[[] for j in range(M)] for i in range(F)]\n",
    "train_times = [[[] for j in range(M)] for i in range(F)]\n",
    "predict_times = [[[] for j in range(M)] for i in range(F)]\n",
    "test_accuracies = [[[] for j in range(M)] for i in range(F)]\n",
    "\n",
    "error_idx = []\n",
    "\n",
    "for i in tqdm(range(3, len(benchmark_suite.tasks))):\n",
    "#     try:\n",
    "    averages, std_errs, preds = OpenML_benchmark(oml_task_id=benchmark_suite.tasks[i], # Select an OpenML task via its task id\n",
    "        n_iterations=n_iterations,\n",
    "        train_test_splits = None, # None forces default OpenML train/test splits\n",
    "        rerfs=rerfs, # A list of random forest types\n",
    "        rerfs_kwargs=rerf_kwargs, # A list of dictionaries of RerF parameters\n",
    "        rerf_param_keyword=rerf_param_keyword, # The keyword of parameter we are changing (i.e. 'trees')\n",
    "        sklearns=sklearns, # A list of sklearn forest types\n",
    "        sklearns_kwargs=sklearn_kwargs, # A list of dictionaries of Sklearn parameters\n",
    "        sklearn_param_keyword=sklearn_param_keyword, # The keyword of parameter we are changing (i.e. 'n_estimators')\n",
    "        param_values = param_values, # A list of values for the parameter we are changing\n",
    "        verbose=False, # Progress bar!\n",
    "        return_predictions=True,\n",
    "        acorn=None\n",
    "    )\n",
    "    for j in range(F):\n",
    "        predictions[j][i] = preds[j]\n",
    "        test_accuracies[j][i] = averages[j][0][0]\n",
    "        train_times[j][i] = averages[j][2][0]\n",
    "        predict_times[j][i] = averages[j][3][0]\n",
    "\n",
    "#     except Exception as e: \n",
    "#         print(e)\n",
    "#         error_idx.append(benchmark_suite.tasks[i])\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "pickle.dump(predictions, open('openMLbenchmark_predictions.pkl', 'wb'))\n",
    "pickle.dump(train_times, open('openMLbenchmark_train_times.pkl', 'wb'))\n",
    "pickle.dump(test_accuracies, open('openMLbenchmark_test_accuracies.pkl', 'wb'))\n",
    "pickle.dump(predict_times, open('openMLbenchmark_predict_times.pkl', 'wb'))\n",
    "pickle.dump(error_idx, open('openMLbenchmark_error_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RerF",
   "language": "python",
   "name": "rerf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
